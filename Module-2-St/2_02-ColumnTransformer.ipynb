{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1:\n",
    "Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary</th>\n",
       "      <th>boro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>103</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>89</td>\n",
       "      <td>Queens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>142</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>Brooklyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>63</td>\n",
       "      <td>Brooklyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>219</td>\n",
       "      <td>Bronx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   salary       boro\n",
       "0     103  Manhattan\n",
       "1      89     Queens\n",
       "2     142  Manhattan\n",
       "3      54   Brooklyn\n",
       "4      63   Brooklyn\n",
       "5     219      Bronx"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame({'salary': [103, 89, 142, 54, 63, 219],\n",
    "                   'boro': ['Manhattan', 'Queens', 'Manhattan', 'Brooklyn', 'Brooklyn', 'Bronx']})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "df = pd.DataFrame({'brand': ['BMW', 'Pepsi', 'Mac', np.nan],\n",
    "                   'category': ['car', 'drink', 'computer', 'Unknown'],\n",
    "                   'num1': [1, 1, 0, 0],\n",
    "                   'target': [0.2, 0.11, 1.34, 1.123]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Column Transformer with Mixed Types\n",
    "\n",
    "\n",
    "This example illustrates how to apply different preprocessing and feature\n",
    "extraction pipelines to different subsets of features, using\n",
    "`ColumnTransformer`. This is particularly handy for the\n",
    "case of datasets that contain heterogeneous data types, since we may want to\n",
    "scale the numeric features and one-hot encode the categorical ones.\n",
    "\n",
    "In this example, the numeric data is standard-scaled after mean-imputation,\n",
    "while the categorical data is one-hot encoded after imputing missing values\n",
    "with a new category (``'missing'``).\n",
    "\n",
    "In addition, we show two different ways to dispatch the columns to the\n",
    "particular pre-processor: by column names and by column data types.\n",
    "\n",
    "Finally, the preprocessing pipeline is integrated in a full prediction pipeline\n",
    "using `Pipeline`, together with a simple classification\n",
    "model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Read the data set `Titanic.csv`\n",
    "Print the column names. Drop 'boat','body','cabin','home.dest','name','ticket','sibsp','parch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Calculate duplicates\n",
    "- Report the size of the dataset\n",
    "- Report if there are any duplicates using `duplicated()` method of pandas dataframe\n",
    "- List all duplicate rows\n",
    "- Drop all duplicate rows using `dop_duplicates` method of pandas dataframe\n",
    "- Report the new shape of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Explore the dataset for missing values\n",
    "- Assign `survived' as target variable `y` and the rest as `X`\n",
    "- Print info about `X` to see if there are any null values and the type of the features\n",
    "- Check if all columns have only alpha numeric characters. To do that you need to use `isalnum()` method of Python which works on strings. Before using this method. we need to change all features into string using `astype('str')`\n",
    "- Replace all `?` characters with `np.nan`\n",
    "- Now, turn 'age','fare' into numeric again using `apply(pd.to_numeric) `\n",
    "- Print the first five samples of `X`\n",
    "- Summarize the number of unique values in each column using `nunique()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Use `ColumnTransformer` by selecting column by names\n",
    "\n",
    "##############################################################################\n",
    " \n",
    " We will train our classifier with the following features:\n",
    "\n",
    " Numeric Features:\n",
    "\n",
    " * ``age``: float;\n",
    " * ``fare``: float.\n",
    "\n",
    " Categorical Features:\n",
    "\n",
    " * ``embarked``: categories encoded as strings ``{'C', 'S', 'Q'}``;\n",
    " * ``sex``: categories encoded as strings ``{'female', 'male'}``;\n",
    " * ``pclass``: ordinal integers ``{1, 2, 3}``.\n",
    "\n",
    " Create the preprocessing pipelines for both numeric and categorical data.\n",
    " - For numeric features, create a pipeline with using imputer with median strategy and standard scaler\n",
    " - For categorical features, use imputer with most frequent and onehot encoder\n",
    " \n",
    " Use columntransformer to transform the features\n",
    " \n",
    " Append the preprocessor to the KNeighborsClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the prediction pipeline in a grid search\n",
    "\n",
    "##############################################################################\n",
    "\n",
    " Grid search can also be performed on the different preprocessing steps\n",
    " defined in the `ColumnTransformer` object, together with the classifier's\n",
    " hyperparameters as part of the `Pipeline`.\n",
    " We will search for both the imputer strategy of the numeric preprocessing\n",
    " and the number of neighbor parameter of the kneighbors classifier using\n",
    " `GridSearchCV`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
